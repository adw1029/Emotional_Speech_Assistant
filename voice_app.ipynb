{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7jMDrR-dOiz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nWlvmlENveFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "wXUe3F4efpPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_audio_file_path = \"/content/drive/MyDrive/CSCI-534/RawData/03-02-01-01-01-01-01.wav\"\n"
      ],
      "metadata": {
        "id": "rV0EQ9ktfsFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy"
      ],
      "metadata": {
        "id": "Q527kO7Au-kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(file_path, target_sr=16000):\n",
        "    # Load the file with librosa, which automatically resamples to the target_sr\n",
        "    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
        "    return audio,sample_rate\n",
        "\n"
      ],
      "metadata": {
        "id": "bxSgrWkOfsZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class multiple_class_model(torch.nn.Module):\n",
        "    def __init__(self,input_size,emo_size=8,hidden_size=256,hidden_size2=128,hidden_size3 = 64,drop_out=0.2, denseN=10,norm_type=\"Batch\"):\n",
        "        super(multiple_class_model, self).__init__()\n",
        "        self.input_size = input_size #X_train.shape[1]\n",
        "        self.emo_size = emo_size\n",
        "        self.hidden1 = hidden_size\n",
        "        self.hidden2 = hidden_size2\n",
        "        self.drop_out = drop_out\n",
        "        self.dense = denseN\n",
        "        if norm_type == \"Batch\":\n",
        "            self.Norm1 = nn.BatchNorm1d(hidden_size)\n",
        "            self.Norm2 = nn.BatchNorm1d(hidden_size3)\n",
        "        elif norm_type == \"Layer\":\n",
        "            self.Norm1 = nn.LayerNorm(hidden_size)\n",
        "            self.Norm2 = nn.LayerNorm(hidden_size3)\n",
        "        self.Conv1 = nn.Conv1d(in_channels=1,\n",
        "                                out_channels=hidden_size,\n",
        "                                kernel_size=5,\n",
        "                                padding='same')\n",
        "        self.maxP = nn.MaxPool1d(kernel_size=8, stride=8, padding=0)\n",
        "\n",
        "        self.Conv2 = nn.Conv1d(in_channels=hidden_size,\n",
        "                                out_channels=hidden_size2,\n",
        "                                kernel_size=5,\n",
        "                                padding='same')\n",
        "\n",
        "        self.Conv3 = nn.Conv1d(in_channels=hidden_size2,\n",
        "                                out_channels=hidden_size3,\n",
        "                                kernel_size=5,\n",
        "                                padding='same')\n",
        "        # self.whole_model = nn.Sequential(\n",
        "        #     nn.Conv1d(in_channels=input_size,\n",
        "        #                         out_channels=hidden_size,\n",
        "        #                         kernel_size=5,\n",
        "        #                         padding='same'),\n",
        "        #     self.Norm1,\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Conv1d(in_channels=hidden_size,\n",
        "        #                         out_channels=hidden_size2,\n",
        "        #                         kernel_size=5,\n",
        "        #                         padding='same'),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(p=drop_out),\n",
        "        #     nn.MaxPool1d(kernel_size=8, stride=8, padding=0),\n",
        "        #     nn.Conv1d(in_channels=hidden_size2,\n",
        "        #                         out_channels=hidden_size3,\n",
        "        #                         kernel_size=5,\n",
        "        #                         padding='same'),\n",
        "        #     self.Norm2,\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Flatten()\n",
        "        # )\n",
        "        self.gender_linear = nn.Linear(hidden_size3 * (input_size//8),2)\n",
        "        self.emo_linear = nn.Linear(hidden_size3 * (input_size//8),emo_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # print(type(x))\n",
        "        x = torch.tensor(x)\n",
        "        x = x.to(device)\n",
        "        #print(x.shape)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.Conv1(x)\n",
        "        x = self.Norm1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.Conv2(x)\n",
        "        x = nn.Dropout(p=self.drop_out)(x)\n",
        "        x = self.maxP(x)\n",
        "        x = self.Conv3(x)\n",
        "        x = self.Norm2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.Flatten()(x)\n",
        "        gender = self.gender_linear(x)\n",
        "        emo = self.emo_linear(x)\n",
        "        final_layer = nn.Softmax(dim=1)\n",
        "        gender = final_layer(gender)\n",
        "        emo = final_layer(emo)\n",
        "        #print(gender)\n",
        "        #print(emo)\n",
        "        return emo,gender"
      ],
      "metadata": {
        "id": "xJvpBKGmuy2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "def init():\n",
        "  yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "  pre_model = hub.load(yamnet_model_handle)\n",
        "  input_n = 1024\n",
        "  model = multiple_class_model(input_n)\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/CSCI-534/best_model.pth\", map_location=torch.device('cpu')))\n",
        "  return pre_model, model"
      ],
      "metadata": {
        "id": "jKsuxSkot0rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(path):\n",
        "  waveform, sr = load_audio(sample_audio_file_path)\n",
        "  scores, embeddings, spectrogram = pre_model(waveform)\n",
        "  print(embeddings)\n",
        "  features = np.mean(embeddings, axis=0)\n",
        "  features = features.reshape((1, 1024))\n",
        "  print(embeddings.shape)\n",
        "  print(features.shape)\n",
        "  emo, gender = model.forward(features)\n",
        "  return emo, gender"
      ],
      "metadata": {
        "id": "iLSd54l-_SQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model, model = init()\n",
        "emo, gender = predict(sample_audio_file_path)\n",
        "print(emo)\n",
        "print(emo.shape)\n",
        "print(gender)\n",
        "print(gender.shape)"
      ],
      "metadata": {
        "id": "-0wF7iMN_Mwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1kSGEYiZ79-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}